{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os, os.path \n",
    "import numpy as np\n",
    "import pickle\n",
    "from glob import glob\n",
    "\n",
    "\"\"\"Change to the data folder\"\"\"\n",
    "train_path = \".\\\\new_train\\\\new2\\\\\"\n",
    "val_path = \".\\\\new_train\\\\ts\\\\\"\n",
    "predict_path = \".\\\\new_val_in\\\\new_val_in\\\\\"\n",
    "dev = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# number of sequences in each dataset\n",
    "# train:205942  val:3200 test: 36272 \n",
    "# sequences sampled at 10HZ rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataset class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArgoverseDataset(Dataset):\n",
    "    \"\"\"Dataset class for Argoverse\"\"\"\n",
    "    def __init__(self, data_path: str, transform=None):\n",
    "        super(ArgoverseDataset, self).__init__()\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "\n",
    "        self.pkl_list = glob(os.path.join(self.data_path, '*'))\n",
    "        self.pkl_list.sort()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pkl_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        pkl_path = self.pkl_list[idx]\n",
    "        with open(pkl_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            \n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "\n",
    "# intialize a dataset\n",
    "train_dataset  = ArgoverseDataset(data_path=train_path)\n",
    "val_dataset = ArgoverseDataset(data_path=val_path)\n",
    "predict_dataset = ArgoverseDataset(data_path=predict_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a loader to enable batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sz = 256\n",
    "\n",
    "def my_collate(batch):\n",
    "    \"\"\" collate lists of samples into batches, create [ batch_sz x agent_sz x seq_len x feature] \"\"\"\n",
    "    inp = [scene['p_in'][scene['track_id'][:,0,0] == scene['agent_id']] for scene in batch]\n",
    "    out = [scene['p_out'][scene['track_id'][:,0,0] == scene['agent_id']] for scene in batch]\n",
    "    inp = torch.tensor(inp).squeeze()\n",
    "    out = torch.tensor(out).squeeze()\n",
    "\n",
    "    base = inp[:,0,:].clone().reshape(-1,1,2)\n",
    "    inp = (inp - base).permute(1,0,2).to(dev)\n",
    "    out = (out - base).permute(1,0,2).to(dev)\n",
    "\n",
    "    return [inp, out]\n",
    "\n",
    "train_loader = DataLoader(train_dataset,batch_size=batch_sz, shuffle = False, collate_fn=my_collate, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset,batch_size=batch_sz, shuffle = False, collate_fn=my_collate, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_batch_sz = 4\n",
    "\n",
    "def my_predict_collate(batch):\n",
    "    \"\"\" collate lists of samples into batches, create [ batch_sz x agent_sz x seq_len x feature] \"\"\"\n",
    "    IDs = [scene['scene_idx'] for scene in batch]\n",
    "\n",
    "    inp = [scene['p_in'][scene['track_id'][:,0,0] == scene['agent_id']] for scene in batch]\n",
    "    inp = torch.tensor(inp).squeeze()\n",
    "    \n",
    "    base = inp[:,0,:].clone().reshape(-1,1,2).cpu()\n",
    "    inp = (inp - base).permute(1,0,2).to(dev)\n",
    "\n",
    "    return [inp, base, IDs]\n",
    "\n",
    "predict_loader = DataLoader(predict_dataset,batch_size=predict_batch_sz, shuffle = False, collate_fn=my_predict_collate, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "79\n",
      "20\n",
      "800\n",
      "torch.Size([19, 256, 2])\n",
      "torch.Size([30, 256, 2])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader))\n",
    "print(len(val_loader))\n",
    "print(len(predict_loader))\n",
    "for i_batch, sample_batch in enumerate(train_loader):\n",
    "    inp, out = sample_batch\n",
    "    print(inp.shape)\n",
    "    #print(inp)\n",
    "    print(out.shape)\n",
    "    #print(out)\n",
    "    print()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_size = 2,\n",
    "                 embedding_size = 128,\n",
    "                 hidden_size = 256,\n",
    "                 n_layers = 4,\n",
    "                 dropout = 0.5):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.linear = nn.Linear(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, n_layers,\n",
    "                           dropout = dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: input batch data, size: [sequence len, batch size, feature size]\n",
    "        for the argoverse trajectory data, size(x) is [19, batch size, 4]\n",
    "        \"\"\"\n",
    "        # embedded: [sequence len, batch size, embedding size]\n",
    "        embedded = self.dropout(F.relu(self.linear(x)))\n",
    "        # you can checkout https://pytorch.org/docs/stable/nn.html?highlight=lstm#torch.nn.LSTM\n",
    "        # for details of the return tensor\n",
    "        # briefly speaking, output coontains the output of last layer for each time step\n",
    "        # hidden and cell contains the last time step hidden and cell state of each layer\n",
    "        # we only use hidden and cell as context to feed into decoder\n",
    "        output, (hidden, cell) = self.rnn(embedded)\n",
    "        # hidden = [n layers * n directions, batch size, hidden size]\n",
    "        # cell = [n layers * n directions, batch size, hidden size]\n",
    "        # the n direction is 1 since we are not using bidirectional RNNs\n",
    "        return hidden, cell\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 output_size = 2,\n",
    "                 embedding_size = 128,\n",
    "                 hidden_size = 256,\n",
    "                 n_layers = 4,\n",
    "                 dropout = 0.5):\n",
    "        super().__init__()\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Linear(output_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, n_layers, dropout = dropout)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        \"\"\"\n",
    "        x : input batch data, size(x): [batch size, feature size]\n",
    "        notice x only has two dimensions since the input is batchs\n",
    "        of last coordinate of observed trajectory\n",
    "        so the sequence length has been removed.\n",
    "        \"\"\"\n",
    "        # add sequence dimension to x, to allow use of nn.LSTM\n",
    "        # after this, size(x) will be [1, batch size, feature size]\n",
    "        x = x.unsqueeze(0)\n",
    "\n",
    "        # embedded = [1, batch size, embedding size]\n",
    "        embedded = self.dropout(F.relu(self.embedding(x)))\n",
    "\n",
    "        #output = [seq len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #seq len and n directions will always be 1 in the decoder, therefore:\n",
    "        #output = [1, batch size, hidden size]\n",
    "        #hidden = [n layers, batch size, hidden size]\n",
    "        #cell = [n layers, batch size, hidden size]\n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "\n",
    "        # prediction = [batch size, output size]\n",
    "        prediction = self.linear(output.squeeze(0))\n",
    "\n",
    "        return prediction, hidden, cell\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "        assert encoder.hidden_size == decoder.hidden_size, \\\n",
    "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            \"Encoder and decoder must have equal number of layers!\"\n",
    "\n",
    "    def forward(self, x, y, teacher_forcing_ratio = 0.5):\n",
    "        \"\"\"\n",
    "        x = [observed sequence len, batch size, feature size]\n",
    "        y = [target sequence len, batch size, feature size]\n",
    "        for our argoverse motion forecasting dataset\n",
    "        observed sequence len is 19, target sequence len is 30\n",
    "        feature size for now is just 4 (x and y)\n",
    "\n",
    "        teacher_forcing_ratio is probability of using teacher forcing\n",
    "        e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[1]\n",
    "        target_len = y.shape[0]\n",
    "        \n",
    "        # tensor to store decoder outputs of each time step\n",
    "        outputs = torch.zeros(y.shape).to(self.device)\n",
    "        \n",
    "        # last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        hidden, cell = self.encoder(x)\n",
    "\n",
    "        # first input to decoder is last coordinates of x\n",
    "        decoder_input = x[-1, :, :]\n",
    "        \n",
    "        for i in range(target_len):\n",
    "            # run decode for one time step\n",
    "            output, hidden, cell = self.decoder(decoder_input, hidden, cell)\n",
    "\n",
    "            # place predictions in a tensor holding predictions for each time step\n",
    "            outputs[i] = output\n",
    "\n",
    "            # decide if we are going to use teacher forcing or not\n",
    "            teacher_forcing = random.random() < teacher_forcing_ratio\n",
    "\n",
    "            # output is the same shape as input, [batch_size, feature size]\n",
    "            # so we can use output directly as input or use true lable depending on\n",
    "            # teacher_forcing is true or not\n",
    "            decoder_input = y[i] if teacher_forcing else output\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = 2\n",
    "OUTPUT_DIM = 2\n",
    "ENC_EMB_DIM = 128\n",
    "DEC_EMB_DIM = 128\n",
    "HID_DIM = 256\n",
    "N_LAYERS = 4\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "model = Seq2Seq(enc, dec, dev).to(dev)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The model has 3,949,826 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, (x, y) in enumerate(dataloader):\n",
    "        if i % 40 == 0:\n",
    "            print(i)\n",
    "        # put data into GPU\n",
    "        x = x.to(dev)\n",
    "        y = y.to(dev)\n",
    "        #print(x.shape)\n",
    "        #print(y.shape)\n",
    "        # zero all param gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # run seq2seq to get predictions\n",
    "        y_pred = model(x.float(), y.float())\n",
    "        \n",
    "        # get loss and compute model trainable params gradients though backpropagation\n",
    "        loss = criterion(y_pred, y.float())\n",
    "        loss.backward()\n",
    "        \n",
    "        # update model params\n",
    "        optimizer.step()\n",
    "        \n",
    "        # add batch loss, since loss is single item tensor\n",
    "        # we can get its value by loss.item()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (x, y) in enumerate(dataloader):\n",
    "            x = x.to(dev)\n",
    "            y = y.to(dev)\n",
    "            \n",
    "            # turn off teacher forcing\n",
    "            y_pred = model(x.float(), y.float(), teacher_forcing_ratio = 0)\n",
    "            \n",
    "            loss = criterion(y_pred, y.float())\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testtt(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (x, y) in enumerate(dataloader):\n",
    "            x = x.float().to(dev)\n",
    "            y = y.float().to(dev)\n",
    "            \n",
    "            # turn off teacher forcing\n",
    "            y_pred = model(x.float(), y, teacher_forcing_ratio = 0)\n",
    "            loss = criterion(y_pred, y)\n",
    "            epoch_loss += loss.item()\n",
    "            if i == 1:\n",
    "                print(x.transpose(0, 1)[0])\n",
    "                print(torch.cat((y.transpose(0, 1)[0], y_pred.transpose(0, 1)[0]), 1))\n",
    "                break\n",
    "                \n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def writefinal(model, dataloader):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    f = open('output.csv', 'a', newline = '\\n')\n",
    "    header = np.array(['ID'] + ['v' + str(i) for i in range(1,61)]).reshape(1,-1)\n",
    "    np.savetxt(f, header,fmt='%s', delimiter=',')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (x, base, IDs) in enumerate(dataloader):\n",
    "            x = x.float().to(dev)\n",
    "            y = torch.zeros(30,x.shape[1],2).float().to(dev)\n",
    "            # turn off teacher forcing\n",
    "            y_pred = model(x, y, teacher_forcing_ratio = 0)\n",
    "            #print(y_pred)\n",
    "            #loss = criterion(y_pred, y)\n",
    "            #epoch_loss += loss.item()\n",
    "            y_pred = np.array(y_pred.squeeze().permute(1,0,2).cpu() + base)\n",
    "            \n",
    "            y_pred = np.hstack((np.array(IDs).reshape(-1,1), y_pred.reshape(-1,60)))\n",
    "            y_pred = pd.DataFrame(y_pred)\n",
    "            y_pred[0] = y_pred[0].astype(int)\n",
    "            y_pred.to_csv(f, header=False, index = False)\n",
    "            if i % 200 == 0:\n",
    "                print(i)\n",
    "            \n",
    "    f.close()\n",
    "    #return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the batch of sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "successfully load previous best model parameters\n",
      "0\n",
      "40\n",
      "Epoch: 01 | Time: 1m 8s\n",
      "\tTrain Loss: 10.534\n",
      "\t Val. Loss: 20.030\n",
      "0\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-9df13aca87cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-9d7ccc5cd5bb>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, dataloader, optimizer, criterion)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mepoch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m40\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    515\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 517\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    555\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 557\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    558\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-a52c7f8e3ec2>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mpkl_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpkl_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpkl_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "N_EPOCHES = 50\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# load previous best model params if exists\n",
    "\n",
    "model_dir = \"saved_models/Seq2Seq\"\n",
    "saved_model_path = model_dir + \"/best_seq2seq.pt\"\n",
    "if os.path.isfile(saved_model_path):\n",
    "    model.load_state_dict(torch.load(saved_model_path))\n",
    "    print(\"successfully load previous best model parameters\")\n",
    "\n",
    "for epoch in range(N_EPOCHES):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_loader, optimizer, criterion)\n",
    "    val_loss = evaluate(model, val_loader, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    mins, secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    print(F'Epoch: {epoch+1:02} | Time: {mins}m {secs}s')\n",
    "    print(F'\\tTrain Loss: {train_loss:.3f}')\n",
    "    print(F'\\t Val. Loss: {val_loss:.3f}')\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        torch.save(model.state_dict(), saved_model_path)\n",
    "\"\"\"\n",
    "model_dir = \"saved_models/Seq2Seq\"\n",
    "saved_model_path = model_dir + \"/best_seq2seq.pt\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "torch.save(model.state_dict(), saved_model_path)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "successfully load previous best model parameters\n",
      "0\n",
      "200\n",
      "400\n",
      "600\n"
     ]
    }
   ],
   "source": [
    "model_dir = \"saved_models/Seq2Seq\"\n",
    "saved_model_path = model_dir + \"/best_seq2seq.pt\"\n",
    "if os.path.isfile(saved_model_path):\n",
    "    model.load_state_dict(torch.load(saved_model_path))\n",
    "    print(\"successfully load previous best model parameters\")\n",
    "writefinal(model, predict_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "successfully load previous best model parameters\n",
      "tensor([[  0.0000,   0.0000],\n",
      "        [  0.0455,  -1.4324],\n",
      "        [  0.2136,  -4.2571],\n",
      "        [  0.4678,  -5.9634],\n",
      "        [  0.6055,  -7.0022],\n",
      "        [  0.8210, -10.4019],\n",
      "        [  0.9728, -11.9463],\n",
      "        [  0.9756, -13.4277],\n",
      "        [  1.1077, -14.7639],\n",
      "        [  1.2413, -17.6355],\n",
      "        [  1.4758, -18.8992],\n",
      "        [  1.6583, -20.6545],\n",
      "        [  1.7065, -23.9756],\n",
      "        [  1.8201, -25.2573],\n",
      "        [  1.9539, -26.6968],\n",
      "        [  1.9980, -28.1606],\n",
      "        [  2.1133, -29.8105],\n",
      "        [  2.2244, -31.4883],\n",
      "        [  2.3692, -34.6809]], device='cuda:0')\n",
      "tensor([[  2.4252, -36.0903,   2.7293, -33.6567],\n",
      "        [  2.5048, -37.6985,   1.4521, -36.2808],\n",
      "        [  2.5717, -39.2524,   0.8036, -37.7156],\n",
      "        [  2.6609, -40.9473,   1.1066, -39.1282],\n",
      "        [  2.6849, -42.4424,   1.4654, -40.8353],\n",
      "        [  2.9122, -45.3745,   1.5371, -42.3214],\n",
      "        [  2.9583, -46.9492,   1.4072, -43.4989],\n",
      "        [  3.0323, -48.4978,   1.6065, -44.9298],\n",
      "        [  3.0925, -50.0769,   1.7336, -46.0377],\n",
      "        [  3.1439, -51.6963,   1.4040, -47.3313],\n",
      "        [  3.2299, -53.2610,   1.0855, -49.5471],\n",
      "        [  3.2521, -56.4761,   1.0397, -51.7263],\n",
      "        [  3.3240, -58.1025,   1.6158, -53.3389],\n",
      "        [  3.3646, -59.6421,   1.9267, -54.3313],\n",
      "        [  3.4198, -61.2378,   1.7722, -55.2516],\n",
      "        [  3.5624, -62.8044,   1.6485, -56.3411],\n",
      "        [  3.5720, -64.3818,   1.9498, -58.1643],\n",
      "        [  3.6573, -65.9109,   1.9861, -61.3066],\n",
      "        [  3.7255, -69.1309,   2.0233, -63.5043],\n",
      "        [  3.7532, -70.6162,   2.2380, -64.2413],\n",
      "        [  3.7592, -72.3628,   2.3821, -64.4817],\n",
      "        [  3.8130, -73.7524,   2.4140, -64.7582],\n",
      "        [  3.8776, -75.4182,   2.3195, -65.8087],\n",
      "        [  3.9401, -78.2705,   2.4908, -68.0070],\n",
      "        [  3.8861, -79.7998,   3.2406, -69.8917],\n",
      "        [  3.9993, -81.3413,   3.1415, -71.5927],\n",
      "        [  3.9910, -82.7295,   2.1298, -73.3687],\n",
      "        [  4.1078, -84.1731,   2.2148, -75.1312],\n",
      "        [  4.1401, -85.9153,   2.1070, -77.0020],\n",
      "        [  4.2476, -87.1670,   1.8624, -78.7738]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model_dir = \"saved_models/Seq2Seq\"\n",
    "saved_model_path = model_dir + \"/best_seq2seq.pt\"\n",
    "if os.path.isfile(saved_model_path):\n",
    "    model.load_state_dict(torch.load(saved_model_path))\n",
    "    print(\"successfully load previous best model parameters\")\n",
    "val_loss = testtt(model, val_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-e34a6e23e746>:7: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  inp = torch.LongTensor(inp)\n",
      "<ipython-input-3-e34a6e23e746>:8: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  out = torch.LongTensor(out)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x216 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "agent_id = 0\n",
    "\n",
    "def show_sample_batch(sample_batch, agent_id):\n",
    "    \"\"\"visualize the trajectory for a batch of samples with a randon agent\"\"\"\n",
    "    inp, out = sample_batch\n",
    "    batch_sz = inp.size(0)\n",
    "    agent_sz = inp.size(1)\n",
    "    \n",
    "    fig, axs = plt.subplots(1,batch_sz, figsize=(15, 3), facecolor='w', edgecolor='k')\n",
    "    fig.subplots_adjust(hspace = .5, wspace=.001)\n",
    "    axs = axs.ravel()   \n",
    "    for i in range(batch_sz):\n",
    "        axs[i].xaxis.set_ticks([])\n",
    "        axs[i].yaxis.set_ticks([])\n",
    "        \n",
    "        # first two feature dimensions are (x,y) positions\n",
    "        axs[i].scatter(inp[i, agent_id,:,0], inp[i, agent_id,:,1])\n",
    "        axs[i].scatter(out[i, agent_id,:,0], out[i, agent_id,:,1])\n",
    "\n",
    "        \n",
    "for i_batch, sample_batch in enumerate(val_loader):\n",
    "    inp, out = sample_batch\n",
    "    \"\"\"TODO:\n",
    "      Deep learning model\n",
    "      training routine\n",
    "    \"\"\"\n",
    "    show_sample_batch(sample_batch, agent_id)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python385jvsc74a57bd007efdcd4b820c98a756949507a4d29d7862823915ec7477944641bea022f4f62",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}